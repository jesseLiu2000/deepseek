nohup: ignoring input
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Loading checkpoint shards:   0%|          | 0/7 [00:00<?, ?it/s]Loading checkpoint shards:  14%|█▍        | 1/7 [00:01<00:10,  1.80s/it]Loading checkpoint shards:  29%|██▊       | 2/7 [00:03<00:09,  1.82s/it]Loading checkpoint shards:  29%|██▊       | 2/7 [00:04<00:12,  2.44s/it]
Traceback (most recent call last):
  File "/scratch0/zx22/zijie/deepseek/eval_raw_model.py", line 31, in <module>
    model = AutoModelForCausalLM.from_pretrained(model_name, torch_dtype=torch.bfloat16, device_map="auto", trust_remote_code=True)
  File "/scratch0/zx22/zijie/miniconda3/envs/deepseek/lib/python3.9/site-packages/transformers/models/auto/auto_factory.py", line 558, in from_pretrained
    return model_class.from_pretrained(
  File "/scratch0/zx22/zijie/miniconda3/envs/deepseek/lib/python3.9/site-packages/transformers/modeling_utils.py", line 3677, in from_pretrained
    ) = cls._load_pretrained_model(
  File "/scratch0/zx22/zijie/miniconda3/envs/deepseek/lib/python3.9/site-packages/transformers/modeling_utils.py", line 4104, in _load_pretrained_model
    new_error_msgs, offload_index, state_dict_index = _load_state_dict_into_meta_model(
  File "/scratch0/zx22/zijie/miniconda3/envs/deepseek/lib/python3.9/site-packages/transformers/modeling_utils.py", line 886, in _load_state_dict_into_meta_model
    set_module_tensor_to_device(model, param_name, param_device, **set_module_kwargs)
  File "/scratch0/zx22/zijie/miniconda3/envs/deepseek/lib/python3.9/site-packages/accelerate/utils/modeling.py", line 399, in set_module_tensor_to_device
    new_value = value.to(device)
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 20.00 MiB. GPU 0 has a total capacty of 31.74 GiB of which 17.38 MiB is free. Including non-PyTorch memory, this process has 16.83 GiB memory in use. Process 66973 has 14.88 GiB memory in use. Of the allocated memory 13.78 GiB is allocated by PyTorch, and 2.75 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
